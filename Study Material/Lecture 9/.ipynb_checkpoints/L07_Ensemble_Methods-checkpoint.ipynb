{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c70d934",
   "metadata": {},
   "source": [
    "# <center> Lecture 09 - Ensemble Methods </center>\n",
    "\n",
    "## Outline\n",
    "- Decision Trees Review\n",
    "- Bagging\n",
    "- Random Forest\n",
    "- Gradient Boost\n",
    "- AdaBoost\n",
    "- XGBoost\n",
    "- Stacking\n",
    "\n",
    "\n",
    "## Decision Tree Review\n",
    "\n",
    "- Without a heavy mathematical background, it is easy to interpret a decision tree (especially if it is small); linear regression requires the understanding of an equation. \n",
    "- Decision trees can graphically depict a higher dimensionality easier than linear regression and still be interpreted by a novice. \n",
    "- The process can easily adapt to qualitative predictors without the need to create and interpret dummy variables. \n",
    "- Decision trees are often believed to reflect a more ‚Äúhuman‚Äù decision-making process as compared to other machine learning methods. \n",
    "- While relatively non-complex among other supervised learning procedures, as a trade-off their predictive accuracy tends to be lower and thus not as competitive. \n",
    "- What if we could combine the benefits of multiple trees in order to yield an overall prediction? Taking the penalty of decreased interpretative value, could this potentially increase our predictive accuracy? \n",
    "    - Bagging \n",
    "    - Random Forests \n",
    "    - Boosting \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c08857b",
   "metadata": {},
   "source": [
    "### Decision Tree Pros and Cons Summary\n",
    "- Pros:\n",
    "    - Interpretability: easier to explain than most other regression methods.\n",
    "    - Easy to handle qualitative predictors.\n",
    "    - Can be displayed graphically.\n",
    "- Cons:\n",
    "    - Instability: a small change in the data may result in a huge different splits.\n",
    "    - Predictive accuracy usually not as good as other approaches.\n",
    "- By aggregating many decision trees, the predictive performance can be improved substantially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f75dd21",
   "metadata": {},
   "source": [
    "### DT classifiers and regressions\n",
    "**Classification**:\n",
    "\n",
    "- Data: S set has L and R subtrees N examples D conditions and K classes\n",
    "- impurity split: $\\frac{L_n}{S}H{L}+\\frac{R_n}{S}H(R)$\n",
    "- the computational complexity grows as N of splits $\\times$ N of classes $=ND\\times NK=DKN^2$\n",
    "- If previous label is already classified correctly, we then can just worry about the next label. \n",
    "- Computation: $NDK$\n",
    "- Reduced from quadratic!\n",
    "\n",
    "**Regression**\n",
    "- we can use any loss function $L(S)=\\frac{1}{N}\\sum_{x,y\\in s}(y-\\mu)^2$ where $\\mu=\\frac{1}{N}\\sum y$\n",
    "- as we get closer to $\\mu$, we face variance $(y-\\mu)^2$ problem!\n",
    "    - we have to limit the depth and number of leaves\n",
    "    \n",
    "### Bias/Variance Tradeoff\n",
    "\n",
    "As the depth of the tree increase:\n",
    "- The training error goes down. (overfit)\n",
    "- The test error does not change too much. (minimum test error does not occur at the exact node)\n",
    "- For interpretability, it's better to choose a smaller tree.\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{split}\n",
    "E[(y-\\hat{y})^2] & =(E[(\\hat{y}-y)])^2+(E[\\hat{y}^2]-(E[\\hat{y}])^2+\\sigma^2 \\\\ \n",
    "error & = bias^2+variance+\\sigma^2\\\\\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "\n",
    "- Bias: error from incorrect model assumptions\n",
    "- Variance: error from random noise\n",
    "- $\\sigma$: noise variance\n",
    "- bias and variance constribut to errors\n",
    "\n",
    "When predictions are independent, we can **reduce the variance by averging the variance**\n",
    "$$Var(\\bar{x})=\\frac{Var(x)}{N}$$\n",
    "\n",
    "Average models to reduce model variance\n",
    "- In any network, the bias can be reduced at the cost of increased variance\n",
    "- In a group of networks, the variance can be reduced at no cost to bias\n",
    "- One problem: only one training set, where do multiple models come from?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a8ef1",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "Basic idea: build different ‚Äúexperts‚Äù and let them **vote**\n",
    "- Advantages:\n",
    "    - Improve predictive performance\n",
    "    - Different types of classifiers can be directly included\n",
    "    - Easy to implement\n",
    "    - Not too much parameter tuning\n",
    "- Disadvantages:\n",
    "    - The combined classifier is not transparent (black box)\n",
    "    - Not a compact representation\n",
    "\n",
    "Predict class label for unseen data by **aggregating a set of predictions** (classifiers learned from the training data)\n",
    "- Bagging (Breiman 1994 ‚ÄúBagging Predictors‚Äù)\n",
    "- Random forests (Breiman 2001 ‚ÄúRandom Forests‚Äù)\n",
    "- Boosting (Freund and Schapire 1995, Friedman et al. 1998)\n",
    "\n",
    "**Large volumes of data**: Sometimes, the amount of data to be analyzed can be too large to be handled by a single classifier.\n",
    "- Partition the data into smaller subsets\n",
    "- Train different classifiers \n",
    "- Combine their outputs using a combination rule\n",
    "\n",
    "**Too little data**: A reasonable sized set of training data is crucial to learn the underlying data distribution.\n",
    "- Draw overlapping random subsets of the available data using resampling techniques\n",
    "- Train different classifiers, creating the ensemble\n",
    "\n",
    "**Divide and conquer**:\n",
    "- The given task may be too complex, or lie outside the space of functions that can be implemented by the chosen classifier method\n",
    "- Appropriate combinations of simple (e.g., linear) classifiers can learn complex (e.g., non-linear) boundaries\n",
    "\n",
    "**Data fusion**:\n",
    "- Several sets of data obtained from different sources, where the nature of features is different (e.g.: categorical and numerical features)\n",
    "- Data from each source can be used to train a different classifier, thus creating an ensemble boundaries\n",
    "\n",
    "**General idea**\n",
    "\n",
    "- A method to generate the individual classifiers of the ensemble\n",
    "- A method for combining the outputs of these classifiers\n",
    "- The individual classifiers must be diverse (errors on different data)\n",
    "- If they make the same errors, such mistakes will be carried into the final prediction\n",
    "- The component classifiers need to be ‚Äúreasonably accurate‚Äù to avoid poor classifiers to obtain the majority of votes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01ad44",
   "metadata": {},
   "source": [
    "## Bagging: *B*ootstrap *Agg*regat*ing*\n",
    "\n",
    "- Take repeated bootstrap samples from training set D (Breiman, 1994)\n",
    "- Bootstrap sampling: Given set D containing N training examples, create D' by drawing N examples at random with replacement from D\n",
    "- **Bagging**:\n",
    "    - Create k bootstrap samples $ùê∑_1,\\dots,ùê∑_ùëò$\n",
    "    - Train distinct classifier on each $ùê∑_ùëñ$\n",
    "    - Classify new instances by majority vote/average\n",
    "    $$h(x)=\\frac{1}{k}\\sum_{j=1}^k h_{D_j}(x)=\\bar{h}(x)$$ \n",
    "    - Goal: reduce the variance $E[(h_D(x)-\\bar{h}(x))^2]$\n",
    "    \n",
    "- To ensure diverse classifiers, the base classifier should be unstable, that is, small changes in the training set should lead to large changes in the classifier output.\n",
    "- Large error reductions have been observed with decision trees and bagging. This is because decision trees are highly sensitive to small perturbations of the training data.\n",
    "- Bagging is not effective with nearest neighbor classifiers. NN classifiers are highly stable with respect to variations of the training data.\n",
    "- When the errors are highly correlated, and bagging becomes ineffective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9b16cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AdaBoostClassifier',\n",
       " 'AdaBoostRegressor',\n",
       " 'BaggingClassifier',\n",
       " 'BaggingRegressor',\n",
       " 'BaseEnsemble',\n",
       " 'ExtraTreesClassifier',\n",
       " 'ExtraTreesRegressor',\n",
       " 'GradientBoostingClassifier',\n",
       " 'GradientBoostingRegressor',\n",
       " 'HistGradientBoostingClassifier',\n",
       " 'HistGradientBoostingRegressor',\n",
       " 'IsolationForest',\n",
       " 'RandomForestClassifier',\n",
       " 'RandomForestRegressor',\n",
       " 'RandomTreesEmbedding',\n",
       " 'StackingClassifier',\n",
       " 'StackingRegressor',\n",
       " 'VotingClassifier',\n",
       " 'VotingRegressor',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_bagging',\n",
       " '_base',\n",
       " '_forest',\n",
       " '_gb',\n",
       " '_gb_losses',\n",
       " '_gradient_boosting',\n",
       " '_hist_gradient_boosting',\n",
       " '_iforest',\n",
       " '_stacking',\n",
       " '_voting',\n",
       " '_weight_boosting']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "dir(ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5b202ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87aa65",
   "metadata": {},
   "source": [
    "BaggingClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "\n",
    "BaggingRegressor: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html\n",
    "\n",
    "For GridSearchCV: https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- When we do GridSearchCV to tune the hyperparameters, the procedure is same for all models:\n",
    "    - provide the name of model\n",
    "    - make an array of choices for hyperparameters of the model want to tune\n",
    "    - give k value for cross validatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7253d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',header=None)\n",
    "df_wine.columns = ['Class label','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols',\n",
    "                    'Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a375244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class label</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class label  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0            1    14.23        1.71  2.43               15.6        127   \n",
       "1            1    13.20        1.78  2.14               11.2        100   \n",
       "2            1    13.16        2.36  2.67               18.6        101   \n",
       "3            1    14.37        1.95  2.50               16.8        113   \n",
       "4            1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f46964",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_wine['Class label']\n",
    "X=df_wine.drop('Class label',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69ad19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7563c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision bag train/test accuracies 1.000/1.000\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=None)\n",
    "bag = BaggingClassifier(base_estimator=tree,n_estimators=500, bootstrap=True, bootstrap_features=True,\n",
    "                        random_state=1)\n",
    "bag = bag.fit(X_train,y_train)\n",
    "y_train_pred = bag.predict(X_train)\n",
    "y_test_pred = bag.predict(X_test)\n",
    "bag_train = accuracy_score(y_train, y_train_pred)\n",
    "bag_test = accuracy_score(y_test, y_test_pred)\n",
    "print('Decision bag train/test accuracies %.3f/%.3f' %(bag_train,bag_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe19863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree train/test accuracies 1.000/0.889\n"
     ]
    }
   ],
   "source": [
    "tree = tree.fit(X_train,y_train)\n",
    "y_train_pred = tree.predict(X_train)\n",
    "y_test_pred = tree.predict(X_test)\n",
    "tree_train = accuracy_score(y_train, y_train_pred)\n",
    "tree_test = accuracy_score(y_test, y_test_pred)\n",
    "print('Decision tree train/test accuracies %.3f/%.3f' %(tree_train,tree_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c61745",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "- Ensemble method specifically designed for decision tree classifiers and regressions.\n",
    "- Two sources of randomness: ‚Äúbagging‚Äù and ‚Äúrandom input vectors‚Äù\n",
    "- Use bootstrap aggregation to train many decision trees.\n",
    "    - Randomly subsample n examples\n",
    "    - Train decision tree on subsample\n",
    "    - Use average or majority vote among learned trees as prediction \n",
    "- Also randomly subsample features: best split at each node is chosen from a random sample of m attributes instead of all attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f0e4ba",
   "metadata": {},
   "source": [
    "### Random Forest - Algorithm\n",
    "For b = 1 to B:\n",
    "   - Draw a bootstrap sample of size ùëÅ from the data D with ùëò attributes. \n",
    "   - Grow a random forest tree ùëáùëè using the bootstrap sample as follows:\n",
    "        - Choose m attributes (ùëö<ùëò) uniformly at random from the data\n",
    "        - Choose the best attribute among the m to split on\n",
    "        - Split on the best attribute and recursively until partitions have fewer than ùë†_ùëöùëñùëõ number of nodes\n",
    "   - Prediction for a new data point x\n",
    "        - Regression: $\\frac{1}{B}\\sum_{b=1}^B T_b(x)$\n",
    "        - Classification: choose the majority class label among $ùëá_1(ùë•),\\dots,ùëá_ùêµ(ùë•)$\n",
    "        \n",
    "**Note**:\n",
    "- Split each training set into two partitions, P and Q, to make the classifier consistent. \n",
    "- Do not grow tree to end. Instead, prune based on the leave out sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b26427",
   "metadata": {},
   "source": [
    "### Bagging vs. Random Forest\n",
    "\n",
    "**Bagging** ‚Äì to average many noisy but approximately unbiased models and reduce the variance. \n",
    "- Averaging benefits since each tree is identically distributed and the expectation of an average of $B$ is the same as the expectation of any one of individual bootstrapped trees.\n",
    "- Identically distributed independent random variables, each with variance $\\sigma^2$, has the variance $\\sigma^2/ùêµ$.\n",
    "- If the variables are dependent with positive pairwise correlation $\\rho$, the variance of the average becomes \n",
    "$\\sigma^2(\\rho+\\frac{1‚àí\\rho}{ùêµ})$\n",
    "\n",
    "**Random Foreset** ‚Äì to improve the variance reduction of bagging by **reducing the correlation between the trees without increasing the variance too much** during the tree-growing process through random selection of the input variables. \n",
    "- Typical values of $m=\\sqrt{k}$ for classification and the minimum node size is one where $m=$ number of features in the model and $k=$ number of features in train set. \n",
    "- Typical values of $ùëö=ùëò/3$  for regression and the minimum node size is 5.\n",
    "- Since $m<k$, it will reduce the correlation between any pair of trees and hence, reduce the variance of the average. \n",
    "- As $m$ decreases, $\\rho\\sigma^2$ decreases but $\\frac{1‚àí\\rho}{ùêµ}\\sigma^2$ increases. \n",
    "- However, as $ùêµ\\to\\infty$, decrease even though the individual tree variance does not change. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7859dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111713f",
   "metadata": {},
   "source": [
    "**Arguments**:\n",
    "Since random forest is combined by decision trees, it also has the arguments of tree:\n",
    "\n",
    "- criterion : default=‚Äùgini‚Äù.\n",
    "- max_depth: default = None.\n",
    "- min_samples_split: default = 2.\n",
    "- min_samples_leaf: default = 1.\n",
    "- n_estimators: The number of trees. default=100.\n",
    "- bootstrap: Whether bootstrap samples are used when building trees. default=true.\n",
    "- oob_score: Whether to use out-of-bag samples to estimate the generalization error. default=false.\n",
    "\n",
    "**Methods**:\n",
    "- fit: Build a forest of trees from the training set (X, y).\n",
    "- score: Return the mean accuracy on the given test data and labels.\n",
    "- predict: Predict class for X.\n",
    "- predict_log_proba: Predict class log-probabilities for X.\n",
    "- predict_proba: Predict class probabilities for X.\n",
    "- set_params: Set the parameters of this estimator.\n",
    "- get_params: Get parameters for this estimator.\n",
    "\n",
    "**Attributes**:\n",
    "- feature_importances_:The feature importances (the higher, the more important the feature).\n",
    "- oob_score_: Score of the training dataset obtained using an out-of-bag estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058fad96",
   "metadata": {},
   "source": [
    "- RandomForestClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- RandomForestRegressor: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a2cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=500,bootstrap=True,random_state=1,oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time RF = RF.fit(X_train,y_train)\n",
    "y_train_pred = RF.predict(X_train)\n",
    "y_test_pred = RF.predict(X_test)\n",
    "RF_train = accuracy_score(y_train, y_train_pred)\n",
    "RF_test = accuracy_score(y_test, y_test_pred)\n",
    "print('Decision RF train/test accuracies %.3f/%.3f' %(RF_train,RF_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd31473",
   "metadata": {},
   "source": [
    "**Out of Bag Sample (OOB)**: \n",
    "- After fitting on the bootstrap sample, it make predictions on the rest of the data set(out of the bootstrap sample).\n",
    "- Can not control the numbers of observations in the out of bag sample.\n",
    "- Almost identical to the obtained K-fold cross validation ‚Äì RF can be fit in one sequence with the cross-validation being performed along the way. \n",
    "- The training can be stopped once the OOB error stabilizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2649d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF.oob_score_ #out of bag score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba142b8",
   "metadata": {},
   "source": [
    "**Variable Importance**:\n",
    "- The improvement in the split is the importance measure attributed to the splitting variable and is accumulated over all the trees in the forest separately for each variable. \n",
    "- RF uses OOB sample to construct a different variable-importance measure to measure the prediction strength of each variable. \n",
    "    - Consider a $b^{th}$ tree, $T_b$:\n",
    "        - The prediction accuracy is recorded. \n",
    "        - The values for jth variable are randomly permuted in the samples and the accuracy gets recomputed. \n",
    "        - The changes of accuracy as a result of permuting is averaged over all trees indicates the importance of jth variable. \n",
    "    - Similar to setting a coefficient to zero in a linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464458c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, importance in zip(X, RF.feature_importances_):\n",
    "    print(name, \"=\", importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7995c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "features = list(X.columns)\n",
    "importances = RF.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e0816",
   "metadata": {},
   "source": [
    "#### Grid search example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1cf9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest = RandomForestClassifier()\n",
    "grid_para_forest = [{\n",
    "    \"n_estimators\": [25, 50, 100],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"min_samples_leaf\": range(1, 10),\n",
    "    \"min_samples_split\": np.linspace(start=2, stop=30, num=15, dtype=int),\n",
    "    \"random_state\": [42]}]\n",
    "grid_search_forest = GridSearchCV(randomForest, grid_para_forest, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "%time grid_search_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_forest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b212b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_forest.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5fcac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The training error is: %.5f\" % (1 - grid_search_forest.score(X_train, y_train)))\n",
    "print(\"The test     error is: %.5f\" % (1 - grid_search_forest.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b35ec",
   "metadata": {},
   "source": [
    "### Boosting Methods\n",
    "\n",
    "Bagging reduces variance by averaging but has little effect on bias.\n",
    "- Can we average and reduce bias? (Michael Kerns in 1988)\n",
    "- Yes, Boosting! (Robert Schapire in 1990)\n",
    "\n",
    "Step fashion approach:\n",
    "- devise computer program for deriving rough rules (weak classifier)\n",
    "- apply procedure to subset of examples and obtain a simple rule \n",
    "- apply to 2nd subset of examples and obtain a 2nd rule\n",
    "- repeat T times\n",
    "\n",
    "How to choose examples on each round?\n",
    "- concentrate on ‚Äúhardest‚Äù examples (those most often misclassified by previous rule)\n",
    "\n",
    "How to combine the rules into single prediction rule?\n",
    "- take (weighted) majority vote of rules\n",
    "\n",
    "boosting = general method of converting rough rules into highly accurate prediction rule \n",
    "- technically assume given ‚Äúweak‚Äù learning algorithm that can consistently find classifiers at least slightly better than random, say, accuracy ¬†55%\n",
    "- given sufficient data, a boosting algorithm can provably construct single classifier with very high accuracy say, 99%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e35365",
   "metadata": {},
   "source": [
    "**Similar to finding the direction without knowning the exact point**:\n",
    "1. starting point: Planet Fitness\n",
    "2. going to walk by looking at the hill. \n",
    "    - make a right turn at every intersection. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd980deb",
   "metadata": {},
   "source": [
    "### Gradient Descent Approach\n",
    "\n",
    "Let $\\mathcal{H}$ be hypothesis class and $H$ be the ensemble classifier,\n",
    "$$l(H)=\\frac{1}{n}\\sum_{i=1}^nl(H(x_i),y_i)$$\n",
    "where $H(x)=\\sum_{t=1}^T\\alpha h_t(x)$ and $h_{t+1}=\\operatorname*{argmin}_{h\\in\\mathcal{H}}l(H_t+\\alpha h_t)$\n",
    "\n",
    "Once $h_{t+1}$ is found, add to the ensemble $H_{t+1}=H_t+\\alpha h_{t+1}$:\n",
    "$$l(H+\\alpha h)\\approx l(H)+\\alpha<\\nabla l(H),h>$$\n",
    "\n",
    "$$\\operatorname*{argmin}_{h\\in\\mathcal{H}}l(H_t+\\alpha h_t)\\approx \\operatorname*{argmin}_{h\\in\\mathcal{H}}<\\nabla l(H),h>$$ \n",
    "\n",
    "Note: \n",
    "- we can ignore the constants when we minimize it when we use Taylor approximation.\n",
    "- the inner product $<\\nabla l(H),h>=\\frac{\\partial l}{\\partial H}h$ \n",
    "$$\\operatorname*{argmin}_{h\\in\\mathcal{H}}l(H_t+\\alpha h_t)=\\operatorname*{argmin}_{h\\in\\mathcal{H}}\\sum_{i=1}\\frac{\\partial l}{\\partial H(x_i)}h(x_i)$$\n",
    "\n",
    "We can do the boosting if we have an algorithm that solves as long as \n",
    "$$h_{t+1}=\\operatorname*{argmin}_{h\\in\\mathcal{H}}\\sum_{i=1}\\frac{\\partial l}{\\partial H(x_i)}h(x_i)<0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52660282",
   "metadata": {},
   "source": [
    "### Gradient Boost\n",
    "\n",
    "- works for both classification and  regression\n",
    "- Weak learners, $h\\in\\mathcal{H}$, are regressors $h(\\vec{x})\\in R$ for all x, typically fixed-depth (between 4-6) regression trees.\n",
    "- Step size $\\alpha$ is fixed to a small constant.\n",
    "- Loss function: Any differentiable convex that decomposes over the sample\n",
    "$$l(H)=\\sum_{i=1}^{n}l(H(\\vec{x}_i))$$ \n",
    "- Must to find a tree $h(\\cdot)$ that maximizes\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{split}\n",
    "h & =\\operatorname*{argmin}_{h\\in\\mathcal{H}}\\sum_{i=1}^{n}r_ih(\\vec{x}_i) \\\\ \n",
    " & =-\\operatorname*{argmin}_{h\\in\\mathcal{H}}\\sum_{i=1}^{n}t_i^2-2t_ih(\\vec{x}_i)+(h(\\vec{x}_i))^2\\\\\n",
    " & =-\\operatorname*{argmin}_{h\\in\\mathcal{H}}\\sum_{i=1}^{n}(h(\\vec{x}_i)-t_i)^2\\\\\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "- assumptions:\n",
    "    1. $\\sum_{i=1}^nh^2(x_i)$ is constant so the normalization of prediction becomes simple and can always decrease $\\sum_{i=1}^nh(x_i)r_i$. \n",
    "    2. CART tree is closed by defining the negative gradient $t_i=-r_i$\n",
    "    \n",
    "- if we use a square loss $l(H)=\\frac{1}{2}\\sum_{i=1}^n(H(x_i)-y_i)^2$,\n",
    "    - the residual $t_i=-\\frac{\\partial l}{\\partial H(x_i)}=y_i-H(\\vec{x}_i)$\n",
    "- we can use any differentiable and convex loss function - then the solution for the next week learner will always be the regression tree minimizing the square loss. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c43f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90eab49",
   "metadata": {},
   "source": [
    "- GradientBoostingClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "- GradientBoostingRegressor: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1,random_state=1)\n",
    "grad = grad.fit(X_train,y_train)\n",
    "y_train_pred = grad.predict(X_train)\n",
    "y_test_pred = grad.predict(X_test)\n",
    "grad_train = accuracy_score(y_train, y_train_pred)\n",
    "grad_test = accuracy_score(y_test, y_test_pred)\n",
    "print('Decision grad train/test accuracies %.3f/%.3f' %(grad_train,grad_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5aa308",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "- Classification: $y_i\\in[+1,‚àí1]$\n",
    "- Weak learners, $h\\in\\mathcal{H}$ are binary, $h(x_i)\\in[‚àí1,+1],\\forall ùë•$\n",
    "- We perform line-search to obtain best step-size $\\alpha$\n",
    "- Use the exponential loss $l(H)=\\sum_{i=1}^ne^{-y_iH(x_i)}$ and the gradient $r_i=-y_ie^{-y_iH(x_i)}$\n",
    "- Notations:\n",
    "    - let $w_i=\\frac{e^{-y_iH(x_i)}}{Z}$ where $Z$ is the normalizing factor $Z=\\sum_{i=1}^ne^{-y_iH(x_i)}$.\n",
    "    - this makes $\\sum_{i}^nw_i=1$ and $w_i$ is the weight. \n",
    "- The next weak learner can be solved by optimization. \n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{split}\n",
    "h & =\\operatorname*{argmin}_{h\\in\\mathcal{H}}\\sum_{i=1}^{n}r_ih(\\vec{x}_i) \\\\ \n",
    " & =\\operatorname*{argmin}_{h\\in\\mathcal{H}}\\sum_{i=1}^{n}-y_ie^{-y_iH(x_i)}h(x_i)\\\\\n",
    " & =\\operatorname*{argmin}_{h\\in\\mathcal{H}}\\sum_{i=1}^{n}-y_iw_ih(x_i)\\\\\n",
    " & =\\operatorname*{argmin}_{h\\in\\mathcal{H}}\\sum_{h(x_i)\\ne y_i}w_i + \\sum_{h(x_i)= y_i}w_i\\\\\n",
    " & =\\operatorname*{argmin}_{h\\in\\mathcal{H}}\\sum_{h(x_i)\\ne y_i}w_i + \\big(1-\\sum_{h(x_i)\\ne y_i}w_i\\big)\\\\\n",
    " & = \\operatorname*{argmin}_{h\\in\\mathcal{H}}\\sum_{h(x_i)\\ne y_i}w_i\\\\\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "- We can find the optimal step-size in the closed form every time we take a \"gradient\" step.\n",
    "    - with give $l$, $\\mathcal{H}$, and $h$:\n",
    "    $$\\alpha=\\operatorname*{argmin}_{\\alpha}l(H+\\alpha h)=\\operatorname*{argmin}_{\\alpha}\\sum_{i=1}^ne^{-y_i[H(x_i)+\\alpha h(x_i)]}$$\n",
    "    - take a derivative r.t. $\\alpha$:\n",
    "$$\\begin{equation}\n",
    "\\begin{split}\n",
    "0 & =\\sum_{i=1}^{n}-y_iH(x_i)e^{-y_i[H(x_i)+\\alpha h(x_i)]}\\\\\n",
    "& = - \\sum_{h(x_i)y_i=1}e^{-y_i[H(x_i)+\\alpha h(x_i)]}+\\sum_{h(x_i)y_i=-1}e^{-y_i[H(x_i)+\\alpha h(x_i)]}\\\\\n",
    "& = - \\sum_{h(x_i)y_i=1}w_ie^{-\\alpha}+\\sum_{h(x_i)y_i=-1}w_ie^{\\alpha}\\\\\n",
    "& = -(1-\\epsilon)e^{-\\alpha}+\\epsilon e^{\\alpha}\\\\\n",
    "\\implies & \\alpha = \\frac{1}{2}\\ln\\frac{1-\\epsilon}{\\epsilon}\n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "- where $\\sum w_i$ is the weight classification error and $\\epsilon<0.5$\n",
    "- After taking a step, we recompute all the weights and then renormalize:\n",
    "    - let the unnormalized weight be $\\hat{w}_i$\n",
    "    $$\\hat{w}_i\\gets\\hat{w}_ie^{-\\alpha h(x_i)y_i}$$\n",
    "    - the normalizer $Z$ becomes\n",
    "    $$Z\\gets Z(2\\sqrt{\\epsilon(1-\\epsilon)})$$\n",
    "    - then \n",
    "    $$w_i\\gets \\frac{w_ie^{-\\alpha h(x_i)y_i}}{2\\sqrt{\\epsilon(1-\\epsilon)}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f3a1e",
   "metadata": {},
   "source": [
    "- AdaBoostClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "- AdaBoostRegressor: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=tree, n_estimators=500, learning_rate=0.1,random_state=1)\n",
    "ada = ada.fit(X_train,y_train)\n",
    "y_train_pred = ada.predict(X_train)\n",
    "y_test_pred = ada.predict(X_test)\n",
    "ada_train = accuracy_score(y_train, y_train_pred)\n",
    "ada_test = accuracy_score(y_test, y_test_pred)\n",
    "print('Decision ada train/test accuracies %.3f/%.3f' %(ada_train,ada_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac2226c",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "- **Regularization**:¬†XGBoost has an option to penalize complex models through both L1 and L2 regularization. \n",
    "- **Handling sparse data**:¬†Missing values or data processing steps like one-hot encoding make data sparse. \n",
    "- **Weighted quantile sketch**:¬†Most existing tree based algorithms can find the split points when the data points are of equal weights (using quantile sketch algorithm). However, they are not equipped to handle weighted data. XGBoost has a distributed weighted quantile sketch algorithm to effectively handle weighted data.\n",
    "- **Block structure for parallel learning**:¬†For faster computing, XGBoost can make use of multiple cores on the CPU. This is possible because of a block structure in its system design. Data is sorted and stored in in-memory units called blocks. Unlike other algorithms, this enables the data layout to be reused by subsequent iterations, instead of computing it again. This feature also serves useful for steps like split finding and column sub-sampling\n",
    "- **Cache awareness**:¬†Non-continuous memory access is required to get the gradient statistics by row index. Hence, XGBoost has been designed to make optimal use of hardware. This is done by allocating internal buffers in each thread, where the gradient statistics can be stored\n",
    "- **Out-of-core computing**:¬†This feature optimizes the available disk space and maximizes its usage when handling huge datasets that do not fit into memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3f974",
   "metadata": {},
   "source": [
    "### XGBoost Regression\n",
    "- Unique regression tree\n",
    "    - start from a single leaf with initial predicited value $p_0=0.5$\n",
    "    - calculate the similarity socre\n",
    "    $$S=\\frac{(\\sum_{i=1}^n(y-p_i))^2}{n+\\lambda}$$\n",
    "    - split by the threshold:\n",
    "        - need to quantify how much better the leaves cluster similar residuals than the root by gain ($G$)\n",
    "        - $G$ is the sum of similarity score of left and right leaves - similarity score of root\n",
    "        $$G=S_L+S_R-S_r$$\n",
    "        - calculate the output value $O$ for each leaf\n",
    "        $$O=\\frac{\\sum_{i=1}^n(y_i-p_i)}{n+\\lambda}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0eface",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb = pd.DataFrame({'x':[10,20,25,35],'y':[-10,7,8,-7]})\n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb['p0'] = [0.5,0.5,0.5,0.5]\n",
    "df_xgb['r0'] = df_xgb['y']-df_xgb['p0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f7ff7",
   "metadata": {},
   "source": [
    "The similarity score:\n",
    "$$ S_0=\\frac{(-10.5+6.5+7.5-7.5)^2}{4+\\lambda}$$\n",
    "- for now, let $\\lambda=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "S0=df_xgb['r0'].sum()**2/(4)\n",
    "print(S0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdad7c2",
   "metadata": {},
   "source": [
    "- We can split by the threshold based on quantile: 15,22.5, and 30. \n",
    "- For each split, we calculate the gain.\n",
    "- the appropriate split is the quantile that gives the highest gain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q=df_xgb['x']+(df_xgb['x'].shift(1)-df_xgb['x'])/2\n",
    "Q=Q[1:]\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_out,SL_out,SR_out = -10000,0,0\n",
    "for t in Q:\n",
    "    df_xgb1=df_xgb[df_xgb['x']<t]\n",
    "    df_xgb2=df_xgb[df_xgb['x']>=t]\n",
    "    SL=df_xgb1['r0'].sum()**2/len(df_xgb1)\n",
    "    SR=df_xgb2['r0'].sum()**2/len(df_xgb2)\n",
    "    G = SL+SR-S0\n",
    "    print('t=',t,'SL=',SL, 'SR=',SR, 'G=',G)\n",
    "    if (G>G_out):\n",
    "        G_out=G\n",
    "        SL_out=SL\n",
    "        SR_out=SR\n",
    "        t_out = t\n",
    "SL1,SR1,G0,t0=SL_out,SR_out,G_out,t_out\n",
    "print(\"the maximum gain:\")\n",
    "print('t=',t0,'SL=',SL1,'SR=',SR1,'G=',G0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf17e7e2",
   "metadata": {},
   "source": [
    "- Shows that the gain is highest when $x\\le15$. \n",
    "- We will have a left subtree with $x\\le15$, $x=[15]$ and a right subtree $x>15$, $x=[20, 25, 30]$.\n",
    "- the output $O$ of a left subtree is $-10.5$.\n",
    "- The next plit is on x=[20, 25, 35]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_out,SL_out,SR_out,t_out = -10000,0,0,0\n",
    "S1 = SR1\n",
    "for t in [22.5, 30]:\n",
    "    df_xgb1=df_xgb[(df_xgb['x']>t0) & (df_xgb['x']<=t)]\n",
    "    df_xgb2=df_xgb[df_xgb['x']>t]\n",
    "    SL=df_xgb1['r0'].sum()**2/len(df_xgb1)\n",
    "    SR=df_xgb2['r0'].sum()**2/len(df_xgb2)\n",
    "    G = SL+SR-S1\n",
    "    if (G>G_out):\n",
    "        SL_out = SL\n",
    "        SR_out = SR\n",
    "        G_out = G\n",
    "        t_out = t\n",
    "t1,SL2,SR2,G1=t_out,SL_out,SR_out,G_out\n",
    "print(\"the maximum gain:\")\n",
    "print('t=',t1,'SL=',SL1,'SR=',SR1,'G=',G1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4793991d",
   "metadata": {},
   "source": [
    "- the next split is when $x\\le30$\n",
    "- a left subtree will have $x=[20,25]$ and a right subtree will have $x=[30]$\n",
    "- the output of a right subtree is $-7.5$\n",
    "- we can still split the left subtree for $x\\le22.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba5142",
   "metadata": {},
   "outputs": [],
   "source": [
    "S2 = SL2\n",
    "SL = 6.5**2\n",
    "SR = 7.5**2\n",
    "G = SL+SR-S2\n",
    "print(\"Similarity of left subtree:\",SL, \"of right subtree:\",SR, \"and gain:\",G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036e0b7",
   "metadata": {},
   "source": [
    "- To prune the tree, we give an arbitrary constant $\\gamma$.\n",
    "- if $G-\\gamma<0$, we remove the branch.\n",
    "- if $G-\\gamma\\ge0$, we keep the branch.\n",
    "- alternatively, we can let $\\lambda>0$ and test if \n",
    "    - $G<0$, remove the branch\n",
    "    - $G>0$, we keep the branch\n",
    "    - let $\\lambda=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e5f8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "S0_new = df_xgb['r0'].sum()**2/(len(df_xgb)+1)\n",
    "G1_new = 10.5**2/2+(6.5+7.5-7.5)**2/4-S0_new\n",
    "S1_new = (6.5)**2/4\n",
    "G2_new=(6.5+7.5)**2/3+7.5**2/2-S1_new\n",
    "S2_new =(6.5+7.5)**2/3\n",
    "G2_new=(6.5)**2/2+7.5**2/2-S2_new\n",
    "print(G2_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5fa76d",
   "metadata": {},
   "source": [
    "- it shows that the new $G<0$.\n",
    "- therefore, we can remove the branch of $x=[20,25]$ split.\n",
    "- how about the output? We only need to worry about $x=[20,25]$ node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfed230",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_xgb['x'].tolist()\n",
    "r = df_xgb['r0'].tolist()\n",
    "L_sum, RL_sum, RR_sum = 0,0,0\n",
    "L_count, RL_count, RR_count = 0,0,0\n",
    "for i in range(0,len(r)):\n",
    "    if x[i]<t0:\n",
    "        L_sum +=r[i]\n",
    "        L_count += 1\n",
    "    elif x[i]<=t1:\n",
    "        RL_sum +=r[i]\n",
    "        RL_count += 1\n",
    "    else :\n",
    "        RR_sum +=r[i]\n",
    "        RR_count += 1\n",
    "\n",
    "O_list = []\n",
    "for i in range(0,len(r)):\n",
    "    if x[i]<t0:\n",
    "        O = L_sum/L_count\n",
    "        O_list.append(O)\n",
    "    elif x[i]<=t1:\n",
    "        O = RL_sum/RL_count\n",
    "        O_list.append(O)\n",
    "    else :\n",
    "        O = RR_sum/RR_count\n",
    "        O_list.append(O)\n",
    "df_xgb['O0']=O_list\n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e9fa54",
   "metadata": {},
   "source": [
    "- The new predict value $p_1=p_0+\\eta O$ where $\\eta=0.3$ by default.\n",
    "- then we can calcualte the new residual $r_1=y-p_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb['p1']=df_xgb['p0']+0.3*df_xgb['O0']\n",
    "df_xgb['r1']=df_xgb['y']-df_xgb['p1']\n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1864a438",
   "metadata": {},
   "source": [
    "### XGBoost Classification\n",
    "- Similiarity Score: \n",
    "    $$S_{i-1}=\\frac{\\sum(y_i-p_i)^2}{\\sum(p_{i-1}(1-p_{i-1}))+\\lambda}$$ \n",
    "- Cover $C$ the minimum number of residuals in each leaf\n",
    "          $$C=\\sum(p_{i-1}(1-p_{i-1}))$$\n",
    "\n",
    "    - in regression: $C=n$\n",
    "    - in classification $C_{min}=1$ by default but we can set $C=0$\n",
    "- Output $O$:\n",
    "$$O_i=\\frac{\\sum(y_i-p_i)}{\\sum(p_{i-1}(1-p_{i-1}))+\\lambda}$$\n",
    "- Prediction:\n",
    "$$\\log{P^i}=\\log{{\\frac{P^{i-1}}{1-P^{i-1}}+\\eta O_i}}$$\n",
    "$$\\to P^i=\\frac{e^{\\log{P^i}}}{1+e^{\\log{P^i}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d234706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb = pd.DataFrame({'x':[10,20,25,35],'y':[0,1,1,0]})\n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ea9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "S0=0\n",
    "p0=0.5\n",
    "df_xgb['r0']=df_xgb['y']-p0\n",
    "G_out,SL_out,SR_out = -10000,0,0\n",
    "\n",
    "Q=[15,22.5,30]\n",
    "for t in Q:\n",
    "    df_xgb1=df_xgb[df_xgb['x']<t]\n",
    "    df_xgb2=df_xgb[df_xgb['x']>=t]\n",
    "    SL=df_xgb1['r0'].sum()**2/(len(df_xgb1)*(p0*(1-p0)))\n",
    "    SR=df_xgb2['r0'].sum()**2/(len(df_xgb2)*(p0*(1-p0)))\n",
    "    G = SL+SR-S0\n",
    "    if (G>G_out):\n",
    "        G_out=G\n",
    "        SL_out=SL\n",
    "        SR_out=SR\n",
    "        t_out = t\n",
    "SL1,SR1,G0,t0=SL_out,SR_out,G_out,t_out\n",
    "print(\"frist split at:\",t0,'has gain',G0)\n",
    "G_out,SL_out,SR_out = -10000,0,0\n",
    "S1=SR1\n",
    "Q=list(set(Q)-set([t0]))\n",
    "for t in Q:\n",
    "    L=df_xgb[(df_xgb['x']>t0)&(df_xgb['x']<=t)]['r0'].tolist()\n",
    "    R=df_xgb[(df_xgb['x']>t)]['r0'].tolist()\n",
    "    SL=np.sum(L)**2/(len(L)*(p0*(1-p0)))\n",
    "    SR=np.sum(R)**2/(len(R)*(p0*(1-p0)))\n",
    "    G = SL+SR-S1 \n",
    "    if (G>G_out):\n",
    "        G_out=G\n",
    "        SL_out=SL\n",
    "        SR_out=SR\n",
    "        t_out = t\n",
    "SL2,SR2,G1,t1=SL_out,SR_out,G_out,t_out\n",
    "print(\"second split at\",t1,'has gain',G1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r0 = df_xgb['r0'].tolist()\n",
    "y = df_xgb['y'].tolist()\n",
    "x = df_xgb['x'].tolist()\n",
    "c0,c1,c2 = 0,0,0\n",
    "r0_sum, r1_sum,r2_sum = 0,0,0\n",
    "for i in range(0,len(r0)):\n",
    "    if x[i]<=t0:\n",
    "        c0 = c0+p0*(1-p0)\n",
    "        r0_sum = r0_sum + r0[i]\n",
    "    elif x[i]<=t1:\n",
    "        c1 = c1+p0*(1-p0)\n",
    "        r1_sum = r1_sum+r0[i]\n",
    "    else:\n",
    "        c2 = c2+p0*(1-p0)\n",
    "        r2_sum = r2_sum+r0[i]\n",
    "O_list = []\n",
    "for i in range(0,len(r0)):\n",
    "    if x[i]<=t0:\n",
    "        O = r0_sum/c0\n",
    "        O_list.append(O)\n",
    "    elif x[i]<=t1:\n",
    "        O = r1_sum/c1\n",
    "        O_list.append(O)\n",
    "    else:\n",
    "        O = r2_sum/c2\n",
    "        O_list.append(O)\n",
    "df_xgb['O0']=O_list\n",
    "df_xgb['P0']=np.log(p0/(1-p0))\n",
    "df_xgb['log(P1)']=df_xgb['P0']+0.3*df_xgb['O0']\n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5cd906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb['P1']=np.exp(df_xgb['log(P1)'])/(1+np.exp(df_xgb['log(P1)']))\n",
    "df_xgb['r1']=df_xgb['y']-df_xgb['P1']\n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ab8b2",
   "metadata": {},
   "source": [
    "### XGBoost Optimization\n",
    "\n",
    "- Regression loss function: $\\frac{1}{2}(y_i-p_i)^2$\n",
    "- Classification loss function: $-[y_i\\log{p_i}-(1-y_i)\\log{1-p_i}]$\n",
    "- Generally\n",
    "$$\\sum_{i=1}^{n}L(y_i,p_i)+\\gamma T + \\frac{1}{2}\\lambda O_i^2=\\sum_{i=1}^nL(y_i,p^0+O_i)+\\frac{1}{2}\\lambda O_i^2$$\n",
    "\n",
    "- the loss function uses a second order Taylor Approximation for the optimal output value:\n",
    "$$L(y,p_i+O)\\approx L(y,p_i)+\\frac{d}{dp_i}L(y,p_i)O_i+\\frac{1}{2}\\big[\\frac{d^2}{dp_i^2}L(y,p_i)\\big]O_i^2$$\n",
    "- for regression:\n",
    "    $$\\begin{equation}\n",
    "        \\begin{split}\n",
    "            \\Big[\\sum_{i=1}^n L(y_i,p_i^0+O)\\Big]+\\frac{1}{2}\\lambda O^2 & = L(y_1,p_1^0)+g_1O+\\frac{1}{2}h_1O^2+\\cdots+L(y_n,p_n^0)+g_nO+\\frac{1}{2}h_nO^2+\\frac{1}{2}\\lambda O^2\\\\\n",
    "                & =L(y_1,p_1^0)+\\sum_{i=1}^n g_iO + \\frac{1}{2}\\sum_{i=1}^n (h_i+\\lambda)O^2\\\\\n",
    "        \\end{split}\n",
    "        \\end{equation}$$\n",
    "\n",
    "    - where $g_i=\\frac{d}{dp_i}L(y_i,p_i)$ and $h_i=\\frac{d^2}{dp_i^2}L(y_i,p_i)$\n",
    "- to find the optimal $O$ value, we make a derivative r.t. $O$ and set equals to 0:\n",
    "$$\\sum_{i=1}^n g_i + \\sum_{i=1}^n(h_i+\\lambda)O=0\\to O=-\\sum_{i=1}^n\\frac{g_i}{h_i+\\lambda}=\\sum_{i=1}^n\\frac{y_i-p_i}{n+\\lambda}$$\n",
    "\n",
    "- by substituting the negative of optimized $O$, $-O$,\n",
    "$$\\begin{equation}\n",
    "\\begin{split}\n",
    "L(y,p_i^0) & =L(y_1,p_1^0)+\\sum_{i=1}^n g_i\\big(\\sum_{i=1}^n\\frac{g_i}{h_i+\\lambda}\\big) - \\frac{1}{2}\\sum_{i=1}^n (h_i+\\lambda)\\big(\\sum_{i=1}^n\\frac{g_i}{h_i+\\lambda}\\big)^2\\\\\n",
    "& =L(y_1,p_1^0)+\\frac{1}{2}\\sum_{i=1}^n\\frac{g_i^2}{h_i+\\lambda}=L(y_1,p_1^0)+\\frac{1}{2}S\n",
    "\\end{split}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007b101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66416d10",
   "metadata": {},
   "source": [
    "**Note** Jupyter notebook does not have XGBoost package installed and it needs to be installed.\n",
    "- For window: https://stackoverflow.com/questions/35510582/how-can-i-install-xgboost-package-in-python-on-windows\n",
    "- For mac:https://machinelearningmastery.com/install-xgboost-python-macos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier( learning_rate =0.1,n_estimators=1000,max_depth=None,min_child_weight=1,gamma=0,\n",
    " subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,seed=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3701b140",
   "metadata": {},
   "source": [
    "- XGBClassifier: https://xgboost.readthedocs.io/en/latest/python/python_intro.html, https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17137d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgb.fit(X_train,y_train)\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "xgb_train = accuracy_score(y_train, y_train_pred)\n",
    "xgb_test = accuracy_score(y_test, y_test_pred)\n",
    "print('Decision xgb train/test accuracies %.3f/%.3f' %(xgb_train,xgb_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f07a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df_wine=df_wine[df_wine['Class label']!=1]\n",
    "y=df_wine['Class label'].values\n",
    "X=df_wine[['Alcohol','OD280/OD315 of diluted wines']].values\n",
    "le=LabelEncoder()\n",
    "y=le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=1,stratify=y)\n",
    "x_min = X_train[:,0].min()-1\n",
    "x_max = X_train[:,0].max()+1\n",
    "y_min = X_train[:,1].min()-1\n",
    "y_max = X_train[:,1].max()+1\n",
    "xx, yy = np.meshgrid(np.arange(x_min,x_max, 0.1),np.arange(y_min,y_max,0.1))\n",
    "f, axarr = plt.subplots(nrows=1, ncols=2,sharex='col',sharey='row',figsize=(8,3))\n",
    "for idx, clf, tt in zip([0,1],[tree,RF],\n",
    "                       ['Decision Tree','Random Forest']):\n",
    "    clf.fit(X_train,y_train)\n",
    "    Z = clf.predict(np.c_[xx.ravel(),yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    axarr[idx].contourf(xx,yy,Z,alpha=0.3)\n",
    "    axarr[idx].scatter(X_train[y_train==0,0],X_train[y_train==0,1],c='blue',marker='^')\n",
    "    axarr[idx].scatter(X_train[y_train==1,0],X_train[y_train==1,1],c='red',marker='o')\n",
    "    axarr[idx].set_title(tt)\n",
    "axarr[0].set_ylabel('Alcohol')\n",
    "plt.text(10.1,-0.8,s='OD280/OD315 of diluted wines',ha='center',va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f632012b",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Decision Trees: need to reduce variance. How?\n",
    "- Bagging: Bootstrap (random subsampling with replacement)\n",
    "- Random Forest\n",
    "    - Bagging method with full decision tree method\n",
    "    - Easy, feature selection, less data pre-processing\n",
    "    - But‚Ä¶ How to reduce bias?\n",
    "- Boosting\n",
    "    - Gradient Boost\n",
    "        - Good for classification & regression\n",
    "        - Simple when we use the square loss function\n",
    "        - Constant small step-size\n",
    "        - Works with any convex differentiable loss function\n",
    "    - AdaBoost\n",
    "        - Only for classification\n",
    "        - Invented first but turned to be one of gradient boost (exponential loss function)\n",
    "        - Need to compute weight and step-size for every iteration \n",
    "    - XGBoost\n",
    "        - Extreme Gradient Boost\n",
    "        - One of most popular ML algorithms now days... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7236ab",
   "metadata": {},
   "source": [
    "# Stacking </center>\n",
    "\n",
    "This file demonstrates the classification stacking ensemble method. The regression stacking can be in a similar manner but using regressors. \n",
    "\n",
    "The stacking using `sklearn` can be done as follow:\n",
    "1. Have a train data set.\n",
    "2. Train the base models. In this example, we use `Logistic regression`, `KNN`, `SVM`, and `DT`.\n",
    "3. Build a meta model via `StackingClassifier`. Use `StackingRegressor` instead for a regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd519fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa6d4d",
   "metadata": {},
   "source": [
    "## 1. Train data\n",
    "We create a binary classification data set (1000 by 20) using `make_classification`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c2057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',header=None)\n",
    "df_wine.columns = ['Class label','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols',\n",
    "                    'Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_wine['Class label']\n",
    "X=df_wine.drop('Class label',axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01eddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Basemodel\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['lr'] = LogisticRegression()\n",
    "    models['knn'] = KNeighborsClassifier()\n",
    "    models['DT'] = DecisionTreeClassifier()\n",
    "    models['svm'] = SVC()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fce3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafaea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X_train, y_train)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ef6f5e",
   "metadata": {},
   "source": [
    "The train model with logistic regression is significantly better than other classifiers indicating that the data is a linear-data and therefore, it is not necessary to do staking. However, let's observe how stacking can improve the classification if we do not include the logistic regression. Any linear models can be used for level 1 mdoel. \n",
    "\n",
    "In level 1 model, I used logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacking():\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    #level0.append(('lr', LogisticRegression()))\n",
    "    level0.append(('knn', KNeighborsClassifier()))\n",
    "    level0.append(('DT', DecisionTreeClassifier()))\n",
    "    level0.append(('svm', SVC()))\n",
    "    # define meta learner model\n",
    "    level1 = LogisticRegression()\n",
    "    # define the stacking ensemble\n",
    "    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188cb581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    models = dict()\n",
    "    #models['lr'] = LogisticRegression()\n",
    "    models['knn'] = KNeighborsClassifier()\n",
    "    models['DT'] = DecisionTreeClassifier()\n",
    "    models['svm'] = SVC()\n",
    "    models['stacking'] = get_stacking()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d3b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b09f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c60b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_stacking()\n",
    "model.fit(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
